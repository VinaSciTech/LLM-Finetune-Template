{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\n# Install required packages\n!pip install unsloth\n!pip install transformers accelerate bitsandbytes peft trl datasets\n!pip install huggingface_hub wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:54:39.749427Z","iopub.execute_input":"2025-03-21T02:54:39.749715Z","iopub.status.idle":"2025-03-21T02:57:37.952656Z","shell.execute_reply.started":"2025-03-21T02:54:39.749684Z","shell.execute_reply":"2025-03-21T02:57:37.951270Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# wandb & huggingface login\n\nfrom huggingface_hub import login\nimport wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"79e6e8b143ebed16f92a172ae979575aacf77b72\"\nwandb.login()\n#wandb.login(\"79e6e8b143ebed16f92a172ae979575aacf77b72\")\nlogin(\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:03:44.354691Z","iopub.execute_input":"2025-03-21T03:03:44.355024Z","iopub.status.idle":"2025-03-21T03:03:44.509608Z","shell.execute_reply.started":"2025-03-21T03:03:44.355002Z","shell.execute_reply":"2025-03-21T03:03:44.508866Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# config username, model, dataset & task name\nmodel_name = \"gemma-3-4b-it\"\ndataset_name = \"nqdhocai/legal_relation_extraction\"\ntask_name = \"relation-extraction\"\nusername = \"nqdhocai\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the peft model\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048\nload_in_4bit = False # equal True if using quantize\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=load_in_4bit,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\n\nprint(f\"Model loaded successfully: DeepSeek R1 8B with Unsloth optimization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:03:45.784192Z","iopub.execute_input":"2025-03-21T03:03:45.784509Z","iopub.status.idle":"2025-03-21T03:04:59.166510Z","shell.execute_reply.started":"2025-03-21T03:03:45.784477Z","shell.execute_reply":"2025-03-21T03:04:59.165601Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5535dbe14a2748708bf3179b10ebc810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322810be078649389a6e05ac44d0c55e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24158ea8ebe64366918a159056b0acf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90dd7ac04c9549789a8813868cdd8403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9ac80cf82c4ca1aaad5f49b725e38f"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.3.17 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully: DeepSeek R1 8B with Unsloth optimization\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Define our specialized counseling prompt template\ncounseling_prompt = \"\"\"\n### Instruct:\n{}\n\n### Context:\n{}\n\n### Response:\n{}\n\"\"\"\n\n# Prepare formatting function\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_counseling_data(examples):\n    instructs = examples['instruct']\n    contexts = examples[\"input\"]\n    responses = examples[\"output\"]\n    texts = []\n\n    for instruct, context, response in zip(instructs, contexts, responses):\n        try:\n            # Format with our counseling template and add EOS token\n            text = counseling_prompt.format(instruct.strip(), context.strip(), response.strip()) + EOS_TOKEN\n            texts.append(text)\n        except:\n            print(instruct, context, response)\n\n    return {\"text\": texts}\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"nqdhocai/legal_relation_extraction\")\n\n\n# Format the dataset for training\nformatted_dataset = dataset.map(\n    format_counseling_data,\n    batched=True,\n    remove_columns=[\"input\", \"output\", \"instruct\", \"category\", 'Unnamed: 0']\n)\n\n# Get the train split\ntrain_dataset = formatted_dataset[\"train\"]\nprint(f\"Dataset prepared with {len(train_dataset)} examples\")\n\n# Display a sample\nprint(\"\\nSample from formatted dataset:\")\nprint(train_dataset[0]['text'][:500] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:08:00.330194Z","iopub.execute_input":"2025-03-21T03:08:00.330522Z","iopub.status.idle":"2025-03-21T03:08:04.261428Z","shell.execute_reply.started":"2025-03-21T03:08:00.330493Z","shell.execute_reply":"2025-03-21T03:08:04.260571Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/55678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532c59430f9b43b1b40d649fab2a8d41"}},"metadata":{}},{"name":"stdout","text":"Cho một câu, hãy chỉ ra các thực thể và mô tả quan hệ giữa chúng Điều 2 4022/QĐ-UBND về việc điều chỉnh, bổ sung kế hoạch sử dụng đất năm 2022 huyện thường tín DANH MỤC CÔNG TRÌNH, DỰ ÁN ĐIỀU CHỈNH, BỔ SUNG KẾ HOẠCH SỬ DỤNG ĐẤT NĂM 2022 HUYỆN THƯỜNG TÍN (Kèm theo Quyết định số 4022/QĐ-UBND ngày 25/10/2022 của UBND Thành phố) None\nPhát hiện các cặp thực thể có quan hệ và phân loại kiểu quan hệ Điều 2 1601/QĐ-UBND  2. None\nXác định cặp thực thể và mối quan hệ giữa chúng dựa trên văn bản Điều 3 2626/QĐ-BTNMT công bố danh mục hệ số phát thải phục vụ kiểm kê khí nhà kính 3 None\nTrích xuất các thực thể từ văn bản và xác định quan hệ giữa chúng Điều 2 45/2022/QĐ-UBND bảng giá nhà ở, công trình xây dựng và giá cấu kiện tổng hợp trên địa bàn tỉnh lâm đồng 2 None\nNhận diện mối quan hệ giữa các thực thể trong câu Điều 7 3237/QĐ-UBND  7 None\nXác định cặp thực thể và mối quan hệ giữa chúng dựa trên văn bản Điều 4 3237/QĐ-UBND  4 None\nTìm kiếm và gán nhãn mối quan hệ giữa các thực thể được nhắc đến Điều 32 3237/QĐ-UBND  32 None\nDataset prepared with 55671 examples\n\nSample from formatted dataset:\n\n### Instruct:\nNhận diện thực thể và xác định cách chúng tương tác trong văn bản\n\n### Context:\nĐiều 11 41/2020/TT-BCA quy định kiểm định nước thải Lật ngược bình chứa mẫu và lắc để kiểm tra độ kín của nắp bình, nếu có nước rỉ ra ngoài thì phải vặn chặt lại, lau khô, lắc kiểm tra lần nữa. Nếu nước vẫn rò rỉ ra ngoài thì phải thay bình chứa khác. Khi thêm hóa chất dạng lỏng, không được quá 05ml hóa chất cho 01 lít mẫu. Để đạt tới pH ≤ 2, có thể lấy lượng chính xác theo tỷ lệ 4ml axit 1:1 hay 2ml a...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:09:31.069162Z","iopub.execute_input":"2025-03-21T03:09:31.069520Z","iopub.status.idle":"2025-03-21T03:09:31.073849Z","shell.execute_reply.started":"2025-03-21T03:09:31.069490Z","shell.execute_reply":"2025-03-21T03:09:31.072820Z"}},"outputs":[{"name":"stdout","text":"55671\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# Configure training\nprint(\"Configuring training...\")\n\n# Show initial GPU memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved initially.\")\n\n# Set up the trainer with optimized parameters for mental health counseling\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Better for precise counseling responses\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_ratio=0.05,  # Gradual warmup\n        max_steps=300,      # Increased for better performance\n        learning_rate=1e-4, # Lower learning rate for sensitive domain\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"cosine\", # Cosine scheduler helps with nuanced learning\n        seed=3407,\n        output_dir=\"mental_health_counselor_model\",\n        report_to=\"none\",\n    ),\n)\n\n# Run training\nprint(\"Starting training...\")\ntrainer_stats = trainer.train()\n\n# Display training stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nprint(f\"Training completed in {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\nprint(f\"Peak memory used: {used_memory} GB ({used_percentage}% of available GPU memory)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:10:06.310652Z","iopub.execute_input":"2025-03-21T03:10:06.310972Z","execution_failed":"2025-03-21T03:31:58.206Z"}},"outputs":[{"name":"stdout","text":"Configuring training...\nGPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n5.748 GB of memory reserved initially.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/55671 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47116ddfa4b64644a7ec7b62e3eeb112"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 55,671 | Num Epochs = 1 | Total steps = 300\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 18/300 19:05 < 5:36:26, 0.01 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.244400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned LoRA adapters\noutput_dir = f\"{username}/{model_name}_{task_name}\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")\n\nmodel.push_to_hub(output_dir)\ntokenizer.push_to_hub(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}