{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\n# Install required packages\n!pip install unsloth\n!pip install transformers accelerate bitsandbytes peft trl datasets\n!pip install huggingface_hub wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T02:54:39.749427Z","iopub.execute_input":"2025-03-21T02:54:39.749715Z","iopub.status.idle":"2025-03-21T02:57:37.952656Z","shell.execute_reply.started":"2025-03-21T02:54:39.749684Z","shell.execute_reply":"2025-03-21T02:57:37.951270Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# wandb & huggingface login\n\nfrom huggingface_hub import login\nimport wandb\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"79e6e8b143ebed16f92a172ae979575aacf77b72\"\nwandb.login()\n#wandb.login(\"79e6e8b143ebed16f92a172ae979575aacf77b72\")\nlogin(\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:03:44.354691Z","iopub.execute_input":"2025-03-21T03:03:44.355024Z","iopub.status.idle":"2025-03-21T03:03:44.509608Z","shell.execute_reply.started":"2025-03-21T03:03:44.355002Z","shell.execute_reply":"2025-03-21T03:03:44.508866Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# config username, model, dataset & task name\nmodel_name = \"gemma-3-4b-it\"\ndataset_name = \"nqdhocai/legal_relation_extraction\"\ntask_name = \"relation-extraction\"\nusername = \"nqdhocai\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the peft model\n\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048\nload_in_4bit = False # equal True if using quantize\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-3-4b-it\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=load_in_4bit,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\n\nprint(f\"Model loaded successfully: DeepSeek R1 8B with Unsloth optimization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:03:45.784192Z","iopub.execute_input":"2025-03-21T03:03:45.784509Z","iopub.status.idle":"2025-03-21T03:04:59.166510Z","shell.execute_reply.started":"2025-03-21T03:03:45.784477Z","shell.execute_reply":"2025-03-21T03:04:59.165601Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.17: Fast Llama patching. Transformers: 4.49.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5535dbe14a2748708bf3179b10ebc810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322810be078649389a6e05ac44d0c55e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24158ea8ebe64366918a159056b0acf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90dd7ac04c9549789a8813868cdd8403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f9ac80cf82c4ca1aaad5f49b725e38f"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.3.17 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully: DeepSeek R1 8B with Unsloth optimization\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Define our specialized counseling prompt template\ncounseling_prompt = \"\"\"\n### Instruct:\n{}\n\n### Context:\n{}\n\n### Response:\n{}\n\"\"\"\n\n# Prepare formatting function\nEOS_TOKEN = tokenizer.eos_token\n\ndef format_counseling_data(examples):\n    instructs = examples['instruct']\n    contexts = examples[\"input\"]\n    responses = examples[\"output\"]\n    texts = []\n\n    for instruct, context, response in zip(instructs, contexts, responses):\n        try:\n            # Format with our counseling template and add EOS token\n            text = counseling_prompt.format(instruct.strip(), context.strip(), response.strip()) + EOS_TOKEN\n            texts.append(text)\n        except:\n            print(instruct, context, response)\n\n    return {\"text\": texts}\n\n# Load the dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"nqdhocai/legal_relation_extraction\")\n\n\n# Format the dataset for training\nformatted_dataset = dataset.map(\n    format_counseling_data,\n    batched=True,\n    remove_columns=[\"input\", \"output\", \"instruct\", \"category\", 'Unnamed: 0']\n)\n\n# Get the train split\ntrain_dataset = formatted_dataset[\"train\"]\nprint(f\"Dataset prepared with {len(train_dataset)} examples\")\n\n# Display a sample\nprint(\"\\nSample from formatted dataset:\")\nprint(train_dataset[0]['text'][:500] + \"...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:08:00.330194Z","iopub.execute_input":"2025-03-21T03:08:00.330522Z","iopub.status.idle":"2025-03-21T03:08:04.261428Z","shell.execute_reply.started":"2025-03-21T03:08:00.330493Z","shell.execute_reply":"2025-03-21T03:08:04.260571Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/55678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532c59430f9b43b1b40d649fab2a8d41"}},"metadata":{}},{"name":"stdout","text":"Cho má»™t cÃ¢u, hÃ£y chá»‰ ra cÃ¡c thá»±c thá»ƒ vÃ  mÃ´ táº£ quan há»‡ giá»¯a chÃºng Äiá»u 2 4022/QÄ-UBND vá» viá»‡c Ä‘iá»u chá»‰nh, bá»• sung káº¿ hoáº¡ch sá»­ dá»¥ng Ä‘áº¥t nÄƒm 2022 huyá»‡n thÆ°á»ng tÃ­n DANH Má»¤C CÃ”NG TRÃŒNH, Dá»° ÃN ÄIá»€U CHá»ˆNH, Bá»” SUNG Káº¾ HOáº CH Sá»¬ Dá»¤NG Äáº¤T NÄ‚M 2022 HUYá»†N THÆ¯á»œNG TÃN (KÃ¨m theo Quyáº¿t Ä‘á»‹nh sá»‘ 4022/QÄ-UBND ngÃ y 25/10/2022 cá»§a UBND ThÃ nh phá»‘) None\nPhÃ¡t hiá»‡n cÃ¡c cáº·p thá»±c thá»ƒ cÃ³ quan há»‡ vÃ  phÃ¢n loáº¡i kiá»ƒu quan há»‡ Äiá»u 2 1601/QÄ-UBND  2. None\nXÃ¡c Ä‘á»‹nh cáº·p thá»±c thá»ƒ vÃ  má»‘i quan há»‡ giá»¯a chÃºng dá»±a trÃªn vÄƒn báº£n Äiá»u 3 2626/QÄ-BTNMT cÃ´ng bá»‘ danh má»¥c há»‡ sá»‘ phÃ¡t tháº£i phá»¥c vá»¥ kiá»ƒm kÃª khÃ­ nhÃ  kÃ­nh 3 None\nTrÃ­ch xuáº¥t cÃ¡c thá»±c thá»ƒ tá»« vÄƒn báº£n vÃ  xÃ¡c Ä‘á»‹nh quan há»‡ giá»¯a chÃºng Äiá»u 2 45/2022/QÄ-UBND báº£ng giÃ¡ nhÃ  á»Ÿ, cÃ´ng trÃ¬nh xÃ¢y dá»±ng vÃ  giÃ¡ cáº¥u kiá»‡n tá»•ng há»£p trÃªn Ä‘á»‹a bÃ n tá»‰nh lÃ¢m Ä‘á»“ng 2 None\nNháº­n diá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ trong cÃ¢u Äiá»u 7 3237/QÄ-UBND  7 None\nXÃ¡c Ä‘á»‹nh cáº·p thá»±c thá»ƒ vÃ  má»‘i quan há»‡ giá»¯a chÃºng dá»±a trÃªn vÄƒn báº£n Äiá»u 4 3237/QÄ-UBND  4 None\nTÃ¬m kiáº¿m vÃ  gÃ¡n nhÃ£n má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ Ä‘Æ°á»£c nháº¯c Ä‘áº¿n Äiá»u 32 3237/QÄ-UBND  32 None\nDataset prepared with 55671 examples\n\nSample from formatted dataset:\n\n### Instruct:\nNháº­n diá»‡n thá»±c thá»ƒ vÃ  xÃ¡c Ä‘á»‹nh cÃ¡ch chÃºng tÆ°Æ¡ng tÃ¡c trong vÄƒn báº£n\n\n### Context:\nÄiá»u 11 41/2020/TT-BCA quy Ä‘á»‹nh kiá»ƒm Ä‘á»‹nh nÆ°á»›c tháº£i Láº­t ngÆ°á»£c bÃ¬nh chá»©a máº«u vÃ  láº¯c Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ kÃ­n cá»§a náº¯p bÃ¬nh, náº¿u cÃ³ nÆ°á»›c rá»‰ ra ngoÃ i thÃ¬ pháº£i váº·n cháº·t láº¡i, lau khÃ´, láº¯c kiá»ƒm tra láº§n ná»¯a. Náº¿u nÆ°á»›c váº«n rÃ² rá»‰ ra ngoÃ i thÃ¬ pháº£i thay bÃ¬nh chá»©a khÃ¡c. Khi thÃªm hÃ³a cháº¥t dáº¡ng lá»ng, khÃ´ng Ä‘Æ°á»£c quÃ¡ 05ml hÃ³a cháº¥t cho 01 lÃ­t máº«u. Äá»ƒ Ä‘áº¡t tá»›i pH â‰¤ 2, cÃ³ thá»ƒ láº¥y lÆ°á»£ng chÃ­nh xÃ¡c theo tá»· lá»‡ 4ml axit 1:1 hay 2ml a...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"print(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:09:31.069162Z","iopub.execute_input":"2025-03-21T03:09:31.069520Z","iopub.status.idle":"2025-03-21T03:09:31.073849Z","shell.execute_reply.started":"2025-03-21T03:09:31.069490Z","shell.execute_reply":"2025-03-21T03:09:31.072820Z"}},"outputs":[{"name":"stdout","text":"55671\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# Configure training\nprint(\"Configuring training...\")\n\n# Show initial GPU memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved initially.\")\n\n# Set up the trainer with optimized parameters for mental health counseling\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Better for precise counseling responses\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_ratio=0.05,  # Gradual warmup\n        max_steps=300,      # Increased for better performance\n        learning_rate=1e-4, # Lower learning rate for sensitive domain\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"cosine\", # Cosine scheduler helps with nuanced learning\n        seed=3407,\n        output_dir=\"mental_health_counselor_model\",\n        report_to=\"none\",\n    ),\n)\n\n# Run training\nprint(\"Starting training...\")\ntrainer_stats = trainer.train()\n\n# Display training stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nprint(f\"Training completed in {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\nprint(f\"Peak memory used: {used_memory} GB ({used_percentage}% of available GPU memory)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T03:10:06.310652Z","iopub.execute_input":"2025-03-21T03:10:06.310972Z","execution_failed":"2025-03-21T03:31:58.206Z"}},"outputs":[{"name":"stdout","text":"Configuring training...\nGPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n5.748 GB of memory reserved initially.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/55671 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47116ddfa4b64644a7ec7b62e3eeb112"}},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 55,671 | Num Epochs = 1 | Total steps = 300\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 18/300 19:05 < 5:36:26, 0.01 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.244400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned LoRA adapters\noutput_dir = f\"{username}/{model_name}_{task_name}\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")\n\nmodel.push_to_hub(output_dir)\ntokenizer.push_to_hub(output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}