{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:06:44.116342Z",
     "iopub.status.busy": "2025-03-19T06:06:44.116148Z",
     "iopub.status.idle": "2025-03-19T06:07:09.110028Z",
     "shell.execute_reply": "2025-03-19T06:07:09.108892Z",
     "shell.execute_reply.started": "2025-03-19T06:06:44.116323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:09.111253Z",
     "iopub.status.busy": "2025-03-19T06:07:09.111006Z",
     "iopub.status.idle": "2025-03-19T06:07:10.343301Z",
     "shell.execute_reply": "2025-03-19T06:07:10.342421Z",
     "shell.execute_reply.started": "2025-03-19T06:07:09.111232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:16:44.499814Z",
     "iopub.status.busy": "2025-03-19T06:16:44.499446Z",
     "iopub.status.idle": "2025-03-19T06:16:44.504476Z",
     "shell.execute_reply": "2025-03-19T06:16:44.503511Z",
     "shell.execute_reply.started": "2025-03-19T06:16:44.499784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "from dataclasses import dataclass, field\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import setup_chat_format, SFTTrainer \n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import torch\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:32.973258Z",
     "iopub.status.busy": "2025-03-19T06:07:32.973053Z",
     "iopub.status.idle": "2025-03-19T06:07:32.977593Z",
     "shell.execute_reply": "2025-03-19T06:07:32.976911Z",
     "shell.execute_reply.started": "2025-03-19T06:07:32.973240Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29.0\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "print(huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:32.978860Z",
     "iopub.status.busy": "2025-03-19T06:07:32.978532Z",
     "iopub.status.idle": "2025-03-19T06:07:33.031453Z",
     "shell.execute_reply": "2025-03-19T06:07:33.030799Z",
     "shell.execute_reply.started": "2025-03-19T06:07:32.978830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainDataConfig:\n",
    "    \"\"\"\n",
    "    Cấu hình dữ liệu huấn luyện.\n",
    "\n",
    "    Attributes:\n",
    "        user (str): Tên vai trò của người dùng trong dữ liệu hội thoại.\n",
    "        assistant (str): Tên vai trò của trợ lý AI trong dữ liệu hội thoại.\n",
    "        system (str): Tên vai trò của hệ thống (nếu có).\n",
    "        \n",
    "        system_prompt (Optional[str]): Prompt hệ thống mặc định (nếu không có cột instruction).\n",
    "        instruction_column (str): Tên cột chứa hướng dẫn trong dataset.\n",
    "        prompt_column (str): Tên cột chứa câu hỏi hoặc đầu vào từ user.\n",
    "        response_column (str): Tên cột chứa phản hồi của trợ lý AI.\n",
    "\n",
    "        dataset_name (Optional[str]): Tên dataset trên Hugging Face Hub (nếu có).\n",
    "        dataset_path (Optional[str]): Đường dẫn dataset nội bộ (nếu có).\n",
    "    \"\"\"\n",
    "\n",
    "    # role config\n",
    "    user: str = \"user\"\n",
    "    assistant: str = \"assistant\"\n",
    "    system: str = \"system\"\n",
    "\n",
    "    # data column config\n",
    "    system_prompt: Optional[str] = None\n",
    "    instruction_column: str = \"instruction\"\n",
    "    prompt_column: str = \"question\"\n",
    "    response_column: str = \"response\"\n",
    "\n",
    "    # Dataset (chỉ chọn một trong hai: `dataset_name` hoặc `dataset_path`)\n",
    "    dataset_name: Optional[str] = None\n",
    "    dataset_path: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:33.033474Z",
     "iopub.status.busy": "2025-03-19T06:07:33.033251Z",
     "iopub.status.idle": "2025-03-19T06:07:33.045782Z",
     "shell.execute_reply": "2025-03-19T06:07:33.044917Z",
     "shell.execute_reply.started": "2025-03-19T06:07:33.033448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    \"\"\"\n",
    "    Class để tải và xử lý dataset cho mô hình NLP.\n",
    "\n",
    "    Attributes:\n",
    "        tokenizer (AutoTokenizer): Tokenizer để xử lý văn bản.\n",
    "        config (TrainDataConfig): Cấu hình chứa thông tin dataset và các tham số khác.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: AutoTokenizer, config: \"TrainDataConfig\"): \n",
    "        \"\"\"\n",
    "        Khởi tạo TrainDataset với tokenizer và config.\n",
    "\n",
    "        Args:\n",
    "            tokenizer (AutoTokenizer): Tokenizer để xử lý văn bản.\n",
    "            config (TrainDataConfig): Cấu hình chứa thông tin dataset và các tham số khác.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer \n",
    "        self.config = config\n",
    "\n",
    "    def _load_dataset(self, path: str = None, dataset_name: str = None):\n",
    "        \"\"\"\n",
    "        Tải dataset từ Hugging Face Datasets hoặc từ file cục bộ (CSV, JSON, XLSX).\n",
    "\n",
    "        Args:\n",
    "            path (str, optional): Đường dẫn đến file dataset (CSV, JSON, XLSX).\n",
    "            dataset_name (str, optional): Tên dataset trên Hugging Face Datasets.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (train_dataset, test_dataset, val_dataset) - Các tập dữ liệu đã được xử lý.\n",
    "        \"\"\"\n",
    "        if dataset_name:\n",
    "            dataset = load_dataset(dataset_name)\n",
    "            train_dataset = dataset.get(\"train\", None)\n",
    "            test_dataset = dataset.get(\"test\", None)\n",
    "            val_dataset = dataset.get(\"val\", None)\n",
    "\n",
    "            if train_dataset:\n",
    "                train_dataset = train_dataset.map(self._format_chat_template, batched=True)\n",
    "            if test_dataset:\n",
    "                test_dataset = test_dataset.map(self._format_chat_template, batched=True)\n",
    "            if val_dataset:\n",
    "                val_dataset = val_dataset.map(self._format_chat_template, batched=True)\n",
    "\n",
    "        elif path:\n",
    "            data_type = path.split(\".\")[-1].lower()\n",
    "            \n",
    "            if data_type == \"csv\":\n",
    "                dataset = pd.read_csv(path)\n",
    "            elif data_type == \"json\":\n",
    "                dataset = pd.read_json(path)\n",
    "            elif data_type == \"xlsx\":\n",
    "                dataset = pd.read_excel(path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please use CSV, JSON, or XLSX.\")\n",
    "\n",
    "            dataset = Dataset.from_pandas(dataset)\n",
    "            dataset = dataset.map(self._format_chat_template, batched=True)\n",
    "            train_dataset, test_dataset, val_dataset = dataset, None, None  # Mặc định gán dataset vào train\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Either 'path' or 'dataset_name' must be provided.\")\n",
    "\n",
    "        return train_dataset, test_dataset, val_dataset\n",
    "\n",
    "    def _format_chat_template(self, data):\n",
    "        \"\"\"\n",
    "        Chuyển đổi dữ liệu thô thành format dùng cho huấn luyện mô hình chatbot.\n",
    "\n",
    "        Args:\n",
    "            data (dict): Dữ liệu input chứa các trường prompt, response.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dữ liệu đã được format lại và tokenized.\n",
    "        \"\"\"\n",
    "        system_prompt = (\n",
    "            data[self.config.instruction_column] \n",
    "            if self.config.instruction_column and self.config.instruction_column in data \n",
    "            else self.config.system_prompt\n",
    "        )\n",
    "\n",
    "        row_json = [\n",
    "            {\"role\": self.config.system, \"content\": system_prompt},\n",
    "            {\"role\": self.config.user, \"content\": data[self.config.prompt_column]},\n",
    "            {\"role\": self.config.assistant, \"content\": data[self.config.response_column]},\n",
    "        ]\n",
    "\n",
    "        data['text'] = self.tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "        return data\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Lấy dataset dựa trên config.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (train_dataset, test_dataset, val_dataset) - Các tập dữ liệu đã được tải và xử lý.\n",
    "        \"\"\"\n",
    "        return self._load_dataset(self.config['dataset_path'], self.config['dataset_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:33.046841Z",
     "iopub.status.busy": "2025-03-19T06:07:33.046615Z",
     "iopub.status.idle": "2025-03-19T06:07:33.058967Z",
     "shell.execute_reply": "2025-03-19T06:07:33.058212Z",
     "shell.execute_reply.started": "2025-03-19T06:07:33.046823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PEFTModelConfig:\n",
    "    \"\"\"\n",
    "    Cấu hình cho mô hình PEFT (Parameter Efficient Fine-Tuning), \n",
    "    bao gồm cấu hình mô hình, LoRA (Low-Rank Adaptation) và BitsAndBytes (bnb) để tối ưu hóa bộ nhớ.\n",
    "\n",
    "    Attributes:\n",
    "        model_name (str): Tên hoặc đường dẫn của mô hình Hugging Face.\n",
    "\n",
    "        # BitsAndBytes config (BNB)\n",
    "        load_in_4bit (bool): Có sử dụng 4-bit quantization không.\n",
    "        bnb_4bit_quant_type (str): Loại lượng tử hóa (vd: \"nf4\" - NormalFloat 4-bit).\n",
    "        bnb_4bit_compute_dtype (torch.dtype): Kiểu dữ liệu tính toán (vd: torch.float16).\n",
    "        bnb_4bit_use_double_quant (bool): Có sử dụng Double Quantization không.\n",
    "\n",
    "        # LoRA config\n",
    "        r (int): Rank của ma trận giảm chiều LoRA.\n",
    "        lora_alpha (int): Hệ số nhân cho trọng số LoRA.\n",
    "        lora_dropout (float): Tỷ lệ dropout trong LoRA.\n",
    "        bias (str): Loại bias được sử dụng (\"none\", \"all\", \"lora_only\").\n",
    "        task_type (str): Loại task của mô hình (vd: \"CAUSAL_LM\", \"SEQ2SEQ_LM\").\n",
    "        target_modules (list): Danh sách các layer sẽ áp dụng LoRA.\n",
    "    \"\"\"\n",
    "\n",
    "    # Model config\n",
    "    model_name: str\n",
    "\n",
    "    # LoRA config\n",
    "    r: int\n",
    "    lora_alpha: int\n",
    "    lora_dropout: float\n",
    "    bias: str\n",
    "    task_type: str\n",
    "    target_modules: list\n",
    "\n",
    "    # BitsAndBytes config\n",
    "    load_in_4bit: bool\n",
    "    bnb_4bit_quant_type: str\n",
    "    bnb_4bit_compute_dtype: torch.dtype = torch.float16\n",
    "    bnb_4bit_use_double_quant: bool = False\n",
    "\n",
    "    \n",
    "\n",
    "    def to_peft_config(self) -> LoraConfig:\n",
    "        \"\"\"\n",
    "        Chuyển đổi sang cấu hình LoraConfig của thư viện PEFT.\n",
    "\n",
    "        Returns:\n",
    "            LoraConfig: Cấu hình LoRA để dùng với mô hình Hugging Face.\n",
    "        \"\"\"\n",
    "        return LoraConfig(\n",
    "            r=self.r,\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            bias=self.bias,\n",
    "            task_type=self.task_type,\n",
    "            target_modules=self.target_modules\n",
    "        )\n",
    "\n",
    "    def to_bnb_config(self) -> BitsAndBytesConfig:\n",
    "        \"\"\"\n",
    "        Chuyển đổi sang cấu hình BitsAndBytesConfig để tối ưu hóa bộ nhớ.\n",
    "\n",
    "        Returns:\n",
    "            BitsAndBytesConfig: Cấu hình BnB để sử dụng với mô hình Hugging Face.\n",
    "        \"\"\"\n",
    "        return BitsAndBytesConfig(\n",
    "            load_in_4bit=self.load_in_4bit,\n",
    "            bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=self.bnb_4bit_compute_dtype,\n",
    "            bnb_4bit_use_double_quant=self.bnb_4bit_use_double_quant,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:42:04.532454Z",
     "iopub.status.busy": "2025-03-19T06:42:04.532150Z",
     "iopub.status.idle": "2025-03-19T06:42:04.539426Z",
     "shell.execute_reply": "2025-03-19T06:42:04.538527Z",
     "shell.execute_reply.started": "2025-03-19T06:42:04.532432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    \"\"\"\n",
    "    Class để tải mô hình với cấu hình PEFT (LoRA) và BitsAndBytes (BNB).\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_peft_model(config: PEFTModelConfig):\n",
    "        \"\"\"\n",
    "        Tải mô hình Hugging Face với cấu hình LoRA và BitsAndBytes.\n",
    "\n",
    "        Args:\n",
    "            config (PEFTModelConfig): Cấu hình mô hình.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (model, tokenizer, peft_config)\n",
    "                - model: Mô hình đã được áp dụng LoRA.\n",
    "                - tokenizer: Tokenizer tương ứng.\n",
    "                - peft_config: Cấu hình LoRA đã được sử dụng.\n",
    "        \"\"\"\n",
    "        \n",
    "        bnb_config = config.to_bnb_config()\n",
    "        peft_config = config.to_peft_config()\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "        tokenizer.chat_template = None  # Vô hiệu hóa template chat nếu có\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "        model = get_peft_model(model, peft_config)\n",
    "\n",
    "        return model, tokenizer, peft_config\n",
    "\n",
    "    @staticmethod\n",
    "    def load_merge_model(finetuned_model, model_name):        \n",
    "        model_path = f\"{model_name.replace('/', '_')}_qlora.pth\"\n",
    "        torch.save(finetuned_model, model_path)\n",
    "\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            use_auth_token=True\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    @staticmethod\n",
    "    def upload_model_to_huggingface(model, username, model_name, full_model=False):\n",
    "        model_name = f\"{model_name.replace('/', '_')}\"\n",
    "        repo_id = f\"{username}/{model_name}\"\n",
    "\n",
    "        if full_model:\n",
    "            model = model.merge_and_unload()\n",
    "        model.save_pretrained(repo_id)\n",
    "        model.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:07:33.076213Z",
     "iopub.status.busy": "2025-03-19T06:07:33.076036Z",
     "iopub.status.idle": "2025-03-19T06:07:33.091984Z",
     "shell.execute_reply": "2025-03-19T06:07:33.091241Z",
     "shell.execute_reply.started": "2025-03-19T06:07:33.076198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    \"\"\"\n",
    "    Cấu hình huấn luyện mô hình sử dụng Hugging Face Transformers.\n",
    "\n",
    "    Attributes:\n",
    "        output_dir (str): Thư mục lưu checkpoint và logs.\n",
    "        per_device_train_batch_size (int): Batch size trên mỗi GPU/TPU.\n",
    "        per_device_eval_batch_size (int): Batch size trên mỗi GPU/TPU cho evaluation.\n",
    "        learning_rate (float): Tốc độ học.\n",
    "        num_train_epochs (int): Số epoch huấn luyện.\n",
    "        weight_decay (float): Hệ số weight decay (regularization).\n",
    "        eval_strategy (str): Chiến lược evaluation (\"steps\" hoặc \"epoch\").\n",
    "        save_strategy (str): Chiến lược lưu checkpoint (\"steps\" hoặc \"epoch\").\n",
    "        logging_dir (str): Thư mục lưu logs TensorBoard.\n",
    "        logging_steps (int): Số bước giữa mỗi lần ghi logs.\n",
    "        save_total_limit (int): Số lượng checkpoint tối đa được giữ lại.\n",
    "        load_best_model_at_end (bool): Có load checkpoint tốt nhất sau training không.\n",
    "        metric_for_best_model (str): Metric để chọn model tốt nhất.\n",
    "        greater_is_better (bool): True nếu metric cao hơn là tốt hơn.\n",
    "        report_to (list): Danh sách nơi gửi logs (vd: [\"wandb\"] hoặc [\"tensorboard\"]).\n",
    "    \"\"\"\n",
    "\n",
    "    output_dir: str\n",
    "    per_device_train_batch_size: int\n",
    "    per_device_eval_batch_size: int\n",
    "    learning_rate: float\n",
    "    num_train_epochs: int\n",
    "    weight_decay: float\n",
    "    eval_strategy: str = \"no\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    logging_dir: str = field(default=\"logs\")\n",
    "    logging_steps: int = 500\n",
    "    save_total_limit: int = 2\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"loss\"\n",
    "    greater_is_better: bool = False\n",
    "    report_to: list = field(default_factory=lambda: [\"wandb\"])  # Sửa lỗi kiểu dữ liệu\n",
    "\n",
    "    def to_training_arguments(self) -> TrainingArguments:\n",
    "        \"\"\"\n",
    "        Chuyển đổi sang TrainingArguments của Hugging Face.\n",
    "\n",
    "        Returns:\n",
    "            TrainingArguments: Đối tượng chứa các tham số huấn luyện.\n",
    "        \"\"\"\n",
    "        return TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            per_device_train_batch_size=self.per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=self.per_device_eval_batch_size,\n",
    "            learning_rate=self.learning_rate,\n",
    "            num_train_epochs=self.num_train_epochs,\n",
    "            weight_decay=self.weight_decay,\n",
    "            eval_strategy=self.eval_strategy,\n",
    "            save_strategy=self.save_strategy,\n",
    "            logging_dir=self.logging_dir,\n",
    "            logging_steps=self.logging_steps,\n",
    "            save_total_limit=self.save_total_limit,\n",
    "            load_best_model_at_end=self.load_best_model_at_end,\n",
    "            metric_for_best_model=self.metric_for_best_model,\n",
    "            greater_is_better=self.greater_is_better,\n",
    "            report_to=self.report_to\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:09:07.075475Z",
     "iopub.status.busy": "2025-03-19T06:09:07.075153Z",
     "iopub.status.idle": "2025-03-19T06:09:07.081855Z",
     "shell.execute_reply": "2025-03-19T06:09:07.081055Z",
     "shell.execute_reply.started": "2025-03-19T06:09:07.075454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Class để huấn luyện mô hình với PEFT và LoRA.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def train(train_config: TrainConfig, peft_model_config: PEFTModelConfig, \n",
    "              finetune_dataset, valid_dataset=None):\n",
    "        \"\"\"\n",
    "        Huấn luyện mô hình sử dụng LoRA.\n",
    "\n",
    "        Args:\n",
    "            train_config (TrainConfig): Cấu hình huấn luyện.\n",
    "            peft_model_config (PEFTModelConfig): Cấu hình mô hình LoRA.\n",
    "            finetune_dataset (Dataset): Tập dữ liệu huấn luyện.\n",
    "            valid_dataset (Dataset, optional): Tập dữ liệu validation. Mặc định là None.\n",
    "        \"\"\"\n",
    "\n",
    "        if \"wandb\" in train_config.report_to:\n",
    "            wandb.init(project=train_config.output_dir.split(\"/\")[-1])  # Lấy tên dự án từ output_dir\n",
    "\n",
    "        model, tokenizer, peft_config = ModelLoader.load_peft_model(peft_model_config)\n",
    "\n",
    "        training_arguments = train_config.to_training_arguments()\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            train_dataset=finetune_dataset,\n",
    "            eval_dataset=valid_dataset,\n",
    "            peft_config=peft_config,\n",
    "            tokenizer=tokenizer,\n",
    "            args=training_arguments,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        best_model_path = os.path.join(train_config.output_dir, \"best\")\n",
    "        trainer.save_model(best_model_path)\n",
    "\n",
    "        if \"wandb\" in train_config.report_to:\n",
    "            wandb.finish()\n",
    "\n",
    "        return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:09:10.523860Z",
     "iopub.status.busy": "2025-03-19T06:09:10.523486Z",
     "iopub.status.idle": "2025-03-19T06:09:10.527522Z",
     "shell.execute_reply": "2025-03-19T06:09:10.526825Z",
     "shell.execute_reply.started": "2025-03-19T06:09:10.523830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model name and Dataset path\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "dataset_name = \"tatsu-lab/alpaca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T06:43:33.270869Z",
     "iopub.status.busy": "2025-03-19T06:43:33.270499Z",
     "iopub.status.idle": "2025-03-19T06:44:18.590012Z",
     "shell.execute_reply": "2025-03-19T06:44:18.588621Z",
     "shell.execute_reply.started": "2025-03-19T06:43:33.270840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b862d556531b>:27: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find 'adapter_config.json' at 'meta-llama_Llama-3.2-1B-Instruct_qlora.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/meta-llama_Llama-3.2-1B-Instruct_qlora.pth/resolve/main/adapter_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                 config_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    258\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    457\u001b[0m             )\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67da67c2-0bce682a169d4dfa2dbc2519;46e20779-df0f-463d-9b82-0878d627d690)\n\nRepository Not Found for url: https://huggingface.co/meta-llama_Llama-3.2-1B-Instruct_qlora.pth/resolve/main/adapter_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-551d25ada993>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_merge_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_model_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mModelLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_model_to_huggingface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nqdhocai\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_model_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-82ac2e3aff24>\u001b[0m in \u001b[0;36mload_merge_model\u001b[0;34m(finetuned_model, model_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             config = PEFT_TYPE_TO_CONFIG_MAPPING[\n\u001b[0;32m--> 479\u001b[0;31m                 PeftConfig._get_peft_type(\n\u001b[0m\u001b[1;32m    480\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                     \u001b[0msubfolder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subfolder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/config.py\u001b[0m in \u001b[0;36m_get_peft_type\u001b[0;34m(cls, model_id, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 )\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find '{CONFIG_NAME}' at '{model_id}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mloaded_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at 'meta-llama_Llama-3.2-1B-Instruct_qlora.pth'"
     ]
    }
   ],
   "source": [
    "train_data_config = TrainDataConfig(\n",
    "    dataset_name=dataset_name,\n",
    "    instruction_column=\"instruction\",\n",
    "    prompt_column=\"input\",\n",
    "    response_column=\"output\"\n",
    ")\n",
    "\n",
    "peft_model_config = PEFTModelConfig(\n",
    "    model_name=model_name,\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    \n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "train_config = TrainConfig(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=[] \n",
    ")\n",
    "\n",
    "dataset = load_dataset(train_data_config.dataset_name)\n",
    "train_dataset = dataset[\"train\"].select(range(100))\n",
    "\n",
    "model, tokenizer = ModelTrainer.train(\n",
    "    train_config=train_config,\n",
    "    peft_model_config=peft_model_config,\n",
    "    finetune_dataset=train_dataset\n",
    ")\n",
    "\n",
    "model, tokenizer = ModelLoader.load_merge_model(model, peft_model_config.model_name)\n",
    "ModelLoader.upload_model_to_huggingface(model, \"nqdhocai\", peft_model_config.model_name, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
